% !Tex root = checkedc.tex

\chapter{Design choices considered but not chosen}
\label{chapter:design-alternatives}

\section{Removing relative alignment}
\label{section:design-alternatives:always-unaligned}

We consider removing the concept of relative alignment from the design to simplify
the design.  Relative alignment is described in Sections~\ref{section:relative-alignment}
and~\ref{section:pointer-cast-results}.  If relative alignment
is removed, however, the compiler has to assume that bounds and pointers are
not relatively aligned.   This would result in more costly bounds checks
as well as significantly more bounds checks in optimized code.

Checks against upper bounds would  take several more instructions.
Given a pointer \var{p} to type \var{T}, \texttt{*\var{p}} accesses the memory from
\var{p} to \texttt{\var{p} + sizeof(\var{T}) - 1}. Given an upper bound \var{ub}, the
upper bounds check for \var{p} becomes \texttt{\var{p} + sizeof(\var{T}) - 1 < \var{ub}},
not \texttt{\var{p} < \var{ub}}.  The
computation of \texttt{\var{p} + sizeof(\var{T}) - 1} would need an overflow check also, 
so there would typically be several extra instructions added to an upper bounds check, 
not just an extra addition.

There will also be more bounds checks because it will be harder for compiler optimizers
to eliminate upper bounds checks in loops.  Most programmers would write code that
strides through an array using a comparison that
\texttt{\var{p} \textless{} \var{ub}}. The comparison 
\texttt{\var{p} < \var{ub}} does not imply
\texttt{\var{p} + sizeof(\var{T}) - 1 < \var{ub}}, so the comparison is
not sufficient for a compiler to optimize away
the upper bounds check. A compiler would have to know that a pointer
and bounds are relatively aligned in order to eliminate the upper bounds
check. It would be hard for a compiler to prove this because it would
require interprocedural or whole-program analysis.

\section{Allowing pointer variables to be assigned values with no relationship to their bounds}

We considered allowing pointer variables to be assigned pointer values
not derived in some way from the object with which their bounds are
associated. The idea would be to avoid unnecessary restrictions on
operations involving pointers and give pointers more leeway to encode
information by modifying bits in pointers.

In this approach, the meaning of a bounds expression would be defined
differently than that given in Section~\ref{section:bounds-declarations}. 
The meaning would be the
following. Given an expression \var{e} with a bounds expression
\texttt{bounds(}\var{lb}\texttt{,} \var{ub}\texttt{)}, let the runtime
values of \var{e}, \var{lb}, and \var{ub} be \var{ev}, \var{lbv},
and \var{ubv}, respectively. If \var{ev} is not null, there will
always exist some object at runtime with the bounds (\var{low},
\var{high}) such that \var{low} \texttt{\textless{}=} \var{lbv} \&\&
\var{ubv} \texttt{\textless{}=} \var{high}. In other words, the
requirement is that expression bounds are always a subrange of the range
of memory for some valid object. This is provided that the value of the
expression with which those bounds are associated is non-null.

The problem with this approach is that it has unexpected consequences
for the bounds that are allowed to be declared for pointer variables.
Any valid pointer bounds could be declared for a variable because there
is no longer a requirement that a pointer stored in the variable is
derived from a pointer to the object associated with the bounds. The
following example would be valid:

\begin{verbatim}
array_ptr<int> x : count(5) = malloc(sizeof(int) * 5);
array_ptr<int> y : bounds(x, x + 5) = malloc(sizeof(int) * 2);
\end{verbatim}

Because the bounds are disassociated from the actual pointer values,
there is much more potential for errors to be made by programmers that
are only detected at runtime. This approach was not pursued further for
this reason.

\section{Address-of and array-to-pointer conversion always produce safe pointer types}

We considered a designed where the address-of operator (\&) and
array-to-pointer type conversion always produced safe pointers. To
preserve compatibility with existing C code, we introduce implicit
conversions from safe pointers to unsafe pointers. We found that we were
not able to preserve backwards compatibility for the address-of operator
and that implicit array-to-pointer conversions required bounds checking.

\subsection{Proposed rules for an address-of operator that produces safe pointer type.}

The address-of operator (\texttt{\&}) applied to an lvalue expression of
type \var{T} would produce a value of type
\ptrT.

Existing C code expects the address-of operator to produce a \var{T} *.
To allow most code to compile without changes, we add an implicit cast
rule: \ptrT can be cast
implicitly to a \var{T} * in those situations where a \var{T}
\texttt{*} type is expected, except for pointer arithmetic operators
that add or subtract a pointer and an integer. Those situations include
pointer assignment, arguments to function calls, return statements, and
conditional expressions. In all these situations a \var{T} \texttt{*}
type must have been declared explicitly in the code already, so this
implicit cast does not introduce unsafe pointer types where none existed
before.

Pointer arithmetic operators are excluded to avoid the silent
introduction of unsafe pointer types and to preserve the value of having
the \ptrT type. If
there were always an implicit cast from \ptrT to \var{T} \texttt{*},
then any expression that uses pointer arithmetic could do pointer
arithmetic on \ptrT
values.

Code takes the address of an array element and immediately does pointer
arithmetic will still fail to type check, introducing a potential
backward compatibility issue:
\begin{verbatim}
f()
{
    int a[10];
    int *x = &a[0] + 5; // &a[0] has type ptr<T>.  Pointer arithmetic is not allowed
    ...
}
\end{verbatim}

We expect this kind of code to be rare because the succinct style is to
use \texttt{a} instead of \texttt{\&a[0]}, but is nonetheless a
possibility, so this proposal still violates the principle of not
changing the meaning of existing C code.

\begin{verbatim}
f()
{
    int a[10];
    int *x = ((int *) &a[0]) + 5; // redundant but OK under old rule
    â€¦
}
\end{verbatim}

\subsection{Proposed rules for conversion of array types to pointer types}

Array types may be complete or incomplete. A complete array type
specifies the number of elements in the array using a constant
expression. In incomplete array type does not specify the number of
elements in the array. Examples of complete array types are int[10]
and int[10][10]. Examples of incomplete array types are
int[] and int[][10].

If the type of an expression or a subexpression is an ``array of
\var{T}'', the following rules would apply. If the array is a complete
type, the type of the expression is altered to
\arrayptrT . If it is
an incomplete type, the type of the expression is altered to \var{T} *.
This alteration does not happen if the expression is an operand of an
address-of operator, \texttt{++}, \texttt{-\/-}, \texttt{sizeof}, or the
left operand of an assignment operator or the `\texttt{.}' operator.

These rules would have an interesting effect for arrays of complete
types: all array references involving those arrays would be bounds
checked. Any address computations involving those arrays will be checked
for overflow also. Because the existing C language definition leaves
out-of-bounds access of arrays of complete type undefined, as well as
the meaning of overflowing address computations undefined, this is
compatible with the existing C language definition.

However, these rules by themselves are problematic for existing C code.
It is common in C code to use array types interchangeably with pointer
types. The rule that complete array types are converted to
\arrayptr\ types could cause problems for such code

\begin{verbatim}
f(int *arg, int len)
{ 
   ...
}

g() {
   int x[10];
   f(x, 10);
}

h() {
   int x[10];
   int *ptr = x;
   f(ptr, 10);
}
\end{verbatim}

To allow existing code to continue to compile unchanged, we adopt the
rule that an \arrayptrT\ can be
implicitly cast to a \var{T} * in situations where a T * type is
expected. Those situations may include pointer assignment, arguments to
function calls, return statements, and conditional expressions. For
conditional, expressions of the form exp1 \texttt{?} exp2 \texttt{:}
exp3, the implicit coercion occurs when exp2 or exp3 has type T * and
the other expression has type
\arrayptrT. These situations do not
include array references and adding or subtracting a pointer type and an
integer. \arrayptrT\ is an acceptable
type for those operations and a coercion to T * is not needed.

We allow \arrayptr\ values to not be within bounds. Because of
this, any implicit conversion of an \arrayptr\ value with a
bounds to an unsafe pointer type must be bounds checked. Otherwise, it
is easy to write ``safe'' code that creates undetected buffer overruns:

\begin{verbatim}
// f looks safe, but does something bad that is undetected before calling unsafe code
f(array_ptr<int> p where bounds(p) == (p, p + 10))
{
    // first argument implicitly converted to int *
    poke(p + random_large_value(), 31415);  
}

void poke(int *p, int val)
{
    *p val
}
\end{verbatim}

The silent introduction of a bounds check at a call to a method violates
the design principles of control and clarity. The implicit conversion
introduces an invisible failure point in a program where one does not
otherwise exist. Pointer arithmetic is not normally bounds checked, so
it is not expected fail.

\section{Alternate definitions of the semantics of bounds declarations}

There are a variety of different possible definitions of the semantics
of bounds declarations. A bounds declaration has the form:

\begin{quote}
\boundsdecl{\var{x}}{\var{bounds-exp}}
\end{quote}

\begin{tabbing}
\var{bounds}\=\var{-exp:} \\
\> \texttt{count(}\var{non-modifying-exp}\texttt{)} \\
\> \bounds{\var{non-modifying-exp}}{\var{non-modifying-exp}} \\
\> \boundsnone \\
\> \boundsany
\end{tabbing}

It may be attached to declarators, parameter declarations, or assignment statements:

\begin{tabbing}
\var{init-}\={declarator:} \\
\>\var{declarator inline-bounds-specifier\textsubscript{opt} where-clause\textsubscript{opt}} \\
\>\var{declarator inline-bounds-specifier\textsubscript{opt} 
where-clause\textsubscript{opt}} \texttt{=} \var{initializer
where-clause\textsubscript{opt}} \\
\>\ldots{} \\
\\
\var{parameter-declaration:} \\
\>\var{declaration-specifiers declarator} \\
\>\var{inline-bounds-specifier\textsubscript{opt} where-clause\textsubscript{opt}} \\
\\
\var{expression-statement:}\\
\>\var{expression\textsubscript{opt} where-clause\textsubscript{opt}}\texttt{;}
\end{tabbing}

The information in the bounds declaration is used at pointer
dereferences involving either (1) the variable or (2) pointers
constructed from the value of the variable.

One design choice for bounds declarations is when
bounds expressions are evaluated.  In the design, evaluation of
bounds expressions is {\em deferred} until bounds checks.  The bounds
expression could be evaluated {\em eagerly} at the point of the bounds declarations.

If bounds expressions in a bounds declaration \boundsdecl{\var{v}}{\var{e}} are evaluated 
eagerly, they must  be evaluated only when \var{v} is non-null.  Bounds are not
meaningful when \var{v} is null.  This prevents eager evaluation from accidentally
causing a runtime failure when a null valued is encountered.    

Consider the code for the use of \texttt{malloc}.   With eager evaluation,
the bounds expressions in {\texttt\bounds{(result, result + size)}} would not
be evaluated if \texttt{malloc} returns \texttt{null}:
\begin{verbatim}
array_ptr<int> result = malloc(size) where result : bounds (result, result + size)
if (result != NULL) {
      ... *result = ...
}
\end{verbatim}

We considered eager evaluation, but rejected it because it would turn \arrayptr
types into \arrayview types.   When a bounds expressions are always eagerly
evaluated, the results need to be stored somewhere so that they can be used
when \var{v} is used.  For local variables, hidden temporary variables could be
introduced.   This breaks the design principle of not introducing hidden
costs, though.  To avoid introducing hidden costs, \arrayptr types could carry
their bounds with them.  However, this turns \arrayptr types into \arrayview types.
For structures, introducing hidden state or converting \arrayptr types to \arrayview
types is especially problematic because it breaks data layout compatibility.

For these reasons, we think it is better to regard bounds declarations as being
compile-time program invariants about the bounds of variables.   Normally, compile-time
program invariants are never evaluated at runtime.  However, in the case of pointers
the program invariants are used to provide runtime bounds safety.

Deferred evaluation of bounds expressions has issues, too, though. First, there
can be problems if a programmer modifies a variable used in a bounds expression
within the extent of a bounds declaration.  Static checking could fail if the
bounds declaration no longer holds.   This would be unexpected behavior for
progammers.  Second, it is a new concept for C programmers.   It could confuse C
programmers.  We recommend using a carefully designed study of programmers to understand 
degree of difficulty of understanding the newe concept.

Because bounds declarations constrain assignments to variables within the scope
of the bounds declarations, we consider several additional definitions of the
extent for bounds declaration:

\begin{compactitem}
    \item The set of statements up to the first assignment to any variable
     used in the bounds declaration. We will call this the ``direct'' set
    \item
      All statements after the bounds declaration where the variables involved in
      the bounds declaration are in scope and a new bounds declaration has not superseded
      the prior bounds declaration. We will call this the ``always'' set.
    \item
      Various sets of program points in between the two extremes of (1) and (2).
      For the sake of discussion, we call this the ``intermediate'' set.
\end{compactitem}
% stopping here for now.

Here is a chart of the different permutations, with comments on their interactions:

\begin{longtable}[c]{p{0.8in}p{2.8in}p{2.8in}}
\toprule
Set of program points & By-value & By-reference \tabularnewline
\midrule
\endhead
Scope & Requires intermediate
        variables                & Static checking may fail at assignments
                                   to variables used in the bounds expressions.\\
Direct & \multicolumn{2}{c}{By-value and by-reference evaluation is
                            indistinguishable.  Because there are no assignments,
                            they produce the same result} \\ \midrule
Always & Introduces hidden temporary  variables that hold bounds value.

May lead to unexpected runtime failures if the variable with bounds is assigned
a pointer to an object outside of its declared bounds & 

Static checking may fail at assignments to variables usesd in bounds expressions.

For a bounds declaration \boundsdecl{\var{v}}{\var{e}}, the bounds of \var{v}
must be bounds using a bounds declarations of \bounds{\var{v}}{\boundsnone} if
variables in e continued to be used after v is no longer used.
\\ \midrule
Intermediate & Introduces hidden temporary variables that hold bounds
value.

May lead to runtime failures if the lhs variable is assigned a pointer
to an object outside of the declared bounds. & \\ \midrule
General comments & We would need to check that all pointer variables in
bounded expressions are non-null before evaluating bounds at
declaration.

Otherwise evaluation may fail at runtime if a variable on the right-hand
side is null. &\tabularnewline
\bottomrule
\end{longtable}
